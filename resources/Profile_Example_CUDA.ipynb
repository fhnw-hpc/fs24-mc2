{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoV0YWpWeX0Y",
    "outputId": "709250cb-0d42-4bbc-9708-b8aefef07c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 17 11:56:26 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| 31%   52C    P0    39W / 170W |   1117MiB /  8192MiB |     17%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2516      G   /usr/lib/xorg/Xorg                492MiB |\n",
      "|    0   N/A  N/A      2818      G   /usr/bin/gnome-shell              116MiB |\n",
      "|    0   N/A  N/A      3643      G   .../xdg-desktop-portal-gnome       10MiB |\n",
      "|    0   N/A  N/A      5554      G   ...oud-3.4.1-x86_64.AppImage        2MiB |\n",
      "|    0   N/A  N/A      7152      G   ...--variations-seed-version       76MiB |\n",
      "|    0   N/A  N/A      8352      G   ...1/usr/lib/firefox/firefox      319MiB |\n",
      "|    0   N/A  N/A     22656      G   /usr/bin/nautilus                  16MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVVwNqwjeYL0",
    "outputId": "da02e775-bfe9-427c-d9f7-82674e8feae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce.cu\n",
    "/* This program will performe a reduce vectorA (size N)\n",
    "* with the + operation.\n",
    "+---------+ \n",
    "|111111111| \n",
    "+---------+\n",
    "     |\n",
    "     N\n",
    "\n",
    "vectorA   = all Ones\n",
    "N = Sum of vectorA\n",
    "*/\n",
    "#include <iostream>\n",
    "#include <sstream>\n",
    "#include <stdlib.h>\n",
    "#include \"cuda_runtime.h\"\n",
    "#include \"device_launch_parameters.h\"\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "\n",
    "// CUDA macro wrapper for checking errors\n",
    "#define gpuErrCheck(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
    "inline void gpuAssert(cudaError_t code, const char* file, int line, bool abort = true)\n",
    "{\n",
    "    if (code != cudaSuccess)\n",
    "    {\n",
    "        std::cout << \"GPUassert: \" << cudaGetErrorString(code) << \" \" << file << \" \" << line << std::endl;\n",
    "        if (abort)\n",
    "        {\n",
    "            exit(code);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// CPU reduce\n",
    "void reduce(int* vectorA, int* sum, int size)\n",
    "{\n",
    "    sum[0] = 0;\n",
    "    for (int i = 0; i < size; i++)\n",
    "        sum[0] += vectorA[i];\n",
    "}\n",
    "\n",
    "\n",
    "// EXERCISE\n",
    "// Read: https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/\n",
    "// Implement the reduce kernel based on the information\n",
    "// of the Nvidia blog post.\n",
    "// Implement both options, using no shared mem at all but global atomics\n",
    "// and using shared mem for the seconds recution phase.\n",
    "__global__ void cudaEvenFasterReduceAddition() {\n",
    "    //ToDo\n",
    "}\n",
    "\n",
    "\n",
    "// Already optimized reduce kernel using shared memory.\n",
    "__global__ void cudaReduceAddition(int* vectorA, int* sum)\n",
    "{\n",
    "    int globalIdx = 2 * blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    extern __shared__ int shmArray[];\n",
    "\n",
    "    shmArray[threadIdx.x] = vectorA[globalIdx];\n",
    "    shmArray[threadIdx.x + blockDim.x] = vectorA[globalIdx + blockDim.x];\n",
    "\n",
    "    for (int stride = blockDim.x; stride; stride >>= 1) {\n",
    "        if (threadIdx.x < stride) {\n",
    "            shmArray[threadIdx.x] += shmArray[threadIdx.x + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        sum[blockIdx.x] = shmArray[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// Compare result vectors\n",
    "int compareResultVec(int* vectorCPU, int* vectorGPU, int size)\n",
    "{\n",
    "    int error = 0;\n",
    "    for (int i = 0; i < size; i++)\n",
    "    {\n",
    "        error += abs(vectorCPU[i] - vectorGPU[i]);\n",
    "    }\n",
    "    if (error == 0)\n",
    "    {\n",
    "        cout << \"No errors. All good!\" << endl;\n",
    "        return 0;\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        cout << \"Accumulated error: \" << error << endl;\n",
    "        return -1;\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Define the size of the vector: 1048576 elements\n",
    "    const int N = 1 << 20;\n",
    "    const int NBR_BLOCK = 512;\n",
    "\n",
    "    // Allocate and prepare input\n",
    "    int* hostVectorA = new int[N];\n",
    "    int hostSumCPU[1];\n",
    "    int hostSumGPU[1];\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        hostVectorA[i] = 1;\n",
    "    }\n",
    "\n",
    "    // Alloc N times size of int at address of deviceVector[A-C]\n",
    "    int* deviceVectorA;\n",
    "    int* deviceSum;\n",
    "    gpuErrCheck(cudaMalloc(&deviceVectorA, N * sizeof(int)));\n",
    "    gpuErrCheck(cudaMalloc(&deviceSum, NBR_BLOCK* sizeof(int)));\n",
    "\n",
    "    // Copy data from host to device\n",
    "    gpuErrCheck(cudaMemcpy(deviceVectorA, hostVectorA, N * sizeof(int), cudaMemcpyHostToDevice));\n",
    "\n",
    "    // Run the vector kernel on the CPU\n",
    "    reduce(hostVectorA, hostSumCPU, N);\n",
    "\n",
    "    // Run kernel on all elements on the GPU\n",
    "    cudaReduceAddition <<<NBR_BLOCK, 1024, 2 * 1024 * sizeof(int)>>> (deviceVectorA, deviceSum);\n",
    "    gpuErrCheck(cudaPeekAtLastError());\n",
    "    cudaReduceAddition <<<1, NBR_BLOCK / 2, NBR_BLOCK * sizeof(int) >> > (deviceSum, deviceSum);\n",
    "    gpuErrCheck(cudaPeekAtLastError());\n",
    "\n",
    "    // Copy the result stored in device_y back to host_y\n",
    "    gpuErrCheck(cudaMemcpy(hostSumGPU, deviceSum, sizeof(int), cudaMemcpyDeviceToHost));\n",
    "\n",
    "    // Check for errors\n",
    "    auto isValid = compareResultVec(hostSumCPU, hostSumGPU, 1);\n",
    "\n",
    "    // Free memory on device\n",
    "    gpuErrCheck(cudaFree(deviceVectorA));\n",
    "    gpuErrCheck(cudaFree(deviceSum));\n",
    "\n",
    "    // Free memory on host\n",
    "    delete[] hostVectorA;\n",
    "\n",
    "    return isValid;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXRAf_-9lub9"
   },
   "source": [
    "**Attention:** If you get a K80, you should compile it like this:\n",
    "`!nvcc -arch=sm_37 -o reduce reduce.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4lLHNUzHef9q"
   },
   "outputs": [],
   "source": [
    "!nvcc -o reduce reduce.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57IzTD97eqDX",
    "outputId": "6aceadaf-3698-4c1a-e9d9-0d69d0c4535c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors. All good!\n"
     ]
    }
   ],
   "source": [
    "!./reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p-YpNLPnMEV"
   },
   "source": [
    "**Profiling on old GPU (K80)**\n",
    "\n",
    "*   `!nvprof --print-gpu-trace ./reduce`\n",
    "*   `!nvprof --analysis-metrics -o reduce_out.nvprof ./reduce`\n",
    "*   --> Use the Visual Profiler\n",
    "\n",
    "**Profiling on newer GPUs**\n",
    "\n",
    "*   `!nsys profile -f true -o reduce_out -t cuda ./reduce`\n",
    "*   `!nsys stats --report gputrace reduce.qdrep`\n",
    "*   `!nsys stats reduce.qdrep`\n",
    "*   `!ncu -f -o reduce --set full ./reduce`\n",
    "*   --> Nvidia Nsight Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPbbe5AaUU4Y"
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FszJbI5nVjz",
    "outputId": "7e836e7a-2ad7-41ab-99a2-906a6fec252f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==253== NVPROF is profiling process 253, command: ./reduce\n",
      "No errors. All good!\n",
      "==253== Profiling application: ./reduce\n",
      "==253== Profiling result:\n",
      "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
      "381.08ms  804.09us                    -               -         -         -         -  4.0000MB  4.8580GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
      "385.04ms  92.031us            (512 1 1)      (1024 1 1)        16        0B  8.0000KB         -           -           -           -     Tesla T4 (0)         1         7  cudaReduceAddition(int*, int*) [114]\n",
      "385.14ms  6.3360us              (1 1 1)       (256 1 1)        16        0B  2.0000KB         -           -           -           -     Tesla T4 (0)         1         7  cudaReduceAddition(int*, int*) [116]\n",
      "385.14ms  2.0160us                    -               -         -         -         -        4B  1.8922MB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
      "\n",
      "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
      "SSMem: Static shared memory allocated per CUDA block.\n",
      "DSMem: Dynamic shared memory allocated per CUDA block.\n",
      "SrcMemType: The type of source memory accessed by memory operation/copy\n",
      "DstMemType: The type of destination memory accessed by memory operation/copy\n"
     ]
    }
   ],
   "source": [
    "!nvprof --print-gpu-trace ./reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZrPgfUgn1DX",
    "outputId": "5459d5d2-fdad-4f19-b5c5-49f516d13d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Warning: Skipping profiling on device 0 since profiling is not supported on devices with compute capability 7.5 and higher.\n",
      "                  Use NVIDIA Nsight Compute for GPU profiling and NVIDIA Nsight Systems for GPU tracing and CPU sampling.\n",
      "                  Refer https://developer.nvidia.com/tools-overview for more details.\n",
      "\n",
      "======== Warning: The option --aggregate-mode on has no effect. The --aggregate-mode <on|off> option applies to --events and --metrics options that follow it.\n",
      "======== Warning: The option --aggregate-mode off has no effect. The --aggregate-mode <on|off> option applies to --events and --metrics options that follow it.\n",
      "==264== NVPROF is profiling process 264, command: ./reduce\n",
      "No errors. All good!\n",
      "==264== Generated result file: /content/reduce_out.nvprof\n"
     ]
    }
   ],
   "source": [
    "!nvprof --analysis-metrics -o reduce_out.nvprof ./reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTzsnXS4UYtc"
   },
   "source": [
    "Use the file *reduce_out.nvprof* with the visual profiler of Nvidia, which you have locally installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrwzKmITUmJp"
   },
   "outputs": [],
   "source": [
    "!nsys profile -f true --stats=true -o reduce_out -t cuda ./reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys stats reduce_out.qdrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
